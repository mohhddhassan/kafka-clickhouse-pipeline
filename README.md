
---

# âš¡ Kafka to ClickHouse Data Pipeline

Hey! I'm Mohamed Hussain â€” currently working as an **Associate Data Engineer Intern** ðŸ‘¨â€ðŸ’»
This repo is my attempt at building a **lightweight real-time pipeline** using **Kafka + ClickHouse** â€” ideal for beginners stepping into the world of streaming and OLAP systems.

If you're curious about how data flows in real-time pipelines, this simple project might help ðŸŽ¯

---

## ðŸ“ Project Structure

| Folder               | Description                                                         |
| -------------------- | ------------------------------------------------------------------- |
| `producer/`          | Python app that generates data and sends it to Kafka and ClickHouse |
| `clickhouse-init/`   | SQL script to create the required table in ClickHouse               |
| `screenshots/`       | Visual proof â€” architecture diagram, Kafka topic, ClickHouse output |
| `docker-compose.yml` | Docker setup to spin up Kafka, Zookeeper, and ClickHouse            |
| `README.md`          | Youâ€™re reading it!                                                  |

---

## ðŸš€ What This Does

* The **producer** generates mock website visit logs (timestamp, URL, country)
* It pushes each record into a **Kafka** topic named `website_visits`
* Simultaneously, the same data is also inserted into a **ClickHouse** table
* All services (Kafka, Zookeeper, ClickHouse) run inside Docker

---

## ðŸŽ¯ Why I Did This

* Understand how real-time systems like **Kafka** and **ClickHouse** interact
* Learn how to write and run a Kafka producer in Python
* Explore OLAP-style storage with ClickHouse
* Build something small but useful âœ¨

---

## ðŸ§  Key Takeaways

* How Kafka brokers and topics work with Python producers
* How to insert records into ClickHouse using Python
* How Docker simplifies environment setup
* How to model basic OLAP tables for real-time ingestion

---

## ðŸ”§ What's Next?

* Add a **Kafka consumer** that reads and inserts into ClickHouse (classic architecture)
* Add logging and error-handling middleware
* Simulate more complex streaming use cases (e.g., log aggregation)
* Add Grafana to visualize real-time metrics from ClickHouse ðŸ“Š

---

## ðŸ™Œ Thanks To

* [ClickHouse](https://clickhouse.com/docs/en/) for the fast OLAP database
* [Confluent Kafka Python](https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html) for the Kafka client
* ClickHouse + Kafka community examples for inspiration

---

## ðŸ‘‹ About Me

**Mohamed Hussain S**
Associate Data Engineer Intern
[LinkedIn](https://linkedin.com/in/hussainmohhdd) | [GitHub](https://github.com/mohhddhassan)

---

> Building in public â€” one stream at a time âš™ï¸

---


You can create a folder named `clickhouse-init/` and inside it, include this SQL file:

### File: `init.sql`

```sql
CREATE TABLE IF NOT EXISTS website_visits (
    timestamp DateTime,
    url String,
    country String
)
ENGINE = MergeTree
ORDER BY (timestamp);
```

This creates a basic OLAP table that organizes visits by timestamp.

---

Let me know if you want the **architecture diagram image** as well â€” I can generate a clean one that shows:

```
+------------+       +--------+        +-------------+
|  Producer  | --->  | Kafka  | --->   | ClickHouse  |
+------------+       +--------+        +-------------+
```

Just say the word and Iâ€™ll generate it.
